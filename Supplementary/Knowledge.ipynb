{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73846307",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4793d",
   "metadata": {},
   "source": [
    "## Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf426a",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "### Gausian random field\n",
    "\n",
    "A true Gaussian random field: generate a height map where each value is random, but related to neighbors → correlated spatial noise.\n",
    "\n",
    "### Sum of randomly places gausian contributers\n",
    "\n",
    "This is what we are doing now to simulate our field:\n",
    "\n",
    "1. Place gausian sources randomly:\n",
    "\n",
    "``` python\n",
    "repellent_positions = [\n",
    "    (np.random.uniform(0.0, 0.45) if np.random.rand() < 0.5 else np.random.uniform(0.55, 1.0), \n",
    "     np.random.uniform(0.0, 0.2)) \n",
    "    for _ in range(num_repellents)\n",
    "]\n",
    "```\n",
    "\n",
    "2. Calculate the sum of gausian points to create a field:\n",
    "\n",
    "``` python\n",
    "repellent_field = np.zeros_like(x)\n",
    "for rx, ry in repellent_positions:\n",
    "    repellent_field += repellent_strength * np.exp(-((x - rx)**2 + (y - ry)**2) / (2 * sigma_repellent**2))\n",
    "```\n",
    "### Gradient descent\n",
    "\n",
    "Considers the steepest change in intensity and takes a step in that direction.\n",
    "\n",
    "__NB sensitive to step size__\n",
    "\n",
    "Let’s say an attractor is at (0.5, 0.2) and the neuron is at (0.5, 0.25).\n",
    "\n",
    "With __small step_size__ = 0.005, the axon moves gradually, allowing a smooth descent toward the attractor.\n",
    "\n",
    "With __large step_size__ = 0.05, it might go too far — say to (0.5, 0.15) — skipping the attractor’s sweet spot, and might even end up exiting the field prematurely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb00c1",
   "metadata": {},
   "source": [
    "## Code:\n",
    "\n",
    "### np.gradient()\n",
    "\n",
    "Used to compute x,y gradient:\n",
    "\n",
    "``` python\n",
    "grad_y, grad_x = np.gradient(noisy_field)\n",
    "```\n",
    "\n",
    "__This tells the axon how to navigate the generated field.__ \n",
    "\n",
    "Differences that can be used:\n",
    "\n",
    "1. Central - what np.gradient is using (except for edge cases)\n",
    "edge cases:\n",
    "2. Forward - compares the current point to the future\n",
    "3. Backward - compares the current to the previous \n",
    "\n",
    "__NB np.gradient uses only the neighbours along the x and y - partial derivatives not all 8 neighbours__\n",
    "\n",
    "uses:\n",
    "\n",
    "•   ↑   •\n",
    "\n",
    "←   ⊗   →\n",
    "\n",
    "•   ↓   •\n",
    "\n",
    "not:\n",
    "\n",
    "↖ ↑ ↗\n",
    "\n",
    "← ⊗ →\n",
    "\n",
    "↙ ↓ ↘\n",
    "\n",
    "---\n",
    "\n",
    "#### Central Differences\n",
    "\n",
    "For **most** points (not at the edges), NumPy uses **central differences**, which means:\n",
    "\n",
    "* For the derivative in `x`, at point `(i, j)`:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x} \\approx \\frac{f(i, j+1) - f(i, j-1)}{2 \\cdot \\Delta x}\n",
    "  $$\n",
    "\n",
    "* For the derivative in `y`, at point `(i, j)`:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial y} \\approx \\frac{f(i+1, j) - f(i-1, j)}{2 \\cdot \\Delta y}\n",
    "  $$\n",
    "\n",
    "So, each derivative uses only **two neighbors**:\n",
    "\n",
    "* Left and right for `x` (columns before and after)\n",
    "* Up and down for `y` (rows above and below)\n",
    "\n",
    "**Not all 8 neighbors** are used — just the **direct neighbors along x and y**.\n",
    "\n",
    "---\n",
    "\\+ smooth movements\n",
    "\n",
    "\\- is it biologically realistic?\n",
    "\n",
    "could write code differently to use all 8 neigbours (see GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368f381",
   "metadata": {},
   "source": [
    "__*Still need to check this code:*__ \n",
    "Could use a variation to compare results if there is time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475017be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_8_neighbors(field):\n",
    "    \"\"\"\n",
    "    Returns a direction vector (dx, dy) at each point in the field based on steepest descent using all 8 neighbors.\n",
    "    \"\"\"\n",
    "    dx = np.zeros_like(field)\n",
    "    dy = np.zeros_like(field)\n",
    "\n",
    "    # Padded version to handle edges\n",
    "    padded = np.pad(field, 1, mode='edge')\n",
    "    \n",
    "    directions = [\n",
    "        (-1, -1), (-1,  0), (-1, +1),\n",
    "        ( 0, -1),          ( 0, +1),\n",
    "        (+1, -1), (+1,  0), (+1, +1),\n",
    "    ]\n",
    "\n",
    "    for i in range(1, field.shape[0]+1):\n",
    "        for j in range(1, field.shape[1]+1):\n",
    "            center = padded[i, j]\n",
    "            max_grad = 0\n",
    "            best_dx = 0\n",
    "            best_dy = 0\n",
    "            \n",
    "            for dy_offset, dx_offset in directions:\n",
    "                neighbor = padded[i + dy_offset, j + dx_offset]\n",
    "                diff = center - neighbor  # higher to lower = descent\n",
    "                grad = diff / np.sqrt(dx_offset**2 + dy_offset**2)  # normalize diagonal distance\n",
    "                \n",
    "                if grad > max_grad:\n",
    "                    max_grad = grad\n",
    "                    best_dx = dx_offset\n",
    "                    best_dy = dy_offset\n",
    "            \n",
    "            dx[i-1, j-1] = best_dx\n",
    "            dy[i-1, j-1] = best_dy\n",
    "\n",
    "    return dx, dy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f8750",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1346db7",
   "metadata": {},
   "source": [
    "## Discussion points:\n",
    "\n",
    "### Isotropy vs antisotropy\n",
    "\n",
    "Our model uses __isotropic gradients__ - diffusion of repellent or is modeled as symetrical in x and y axis. This would be the case in a uniform medium, however tissues are not so uniform. Tissue fibers or cell bodies are influencing the diffusion in real biological systems. __Antisotropic__ gradients might better reflect this. \n",
    "\n",
    "---\n",
    "\n",
    "### Axon navigation\n",
    "\n",
    "#### 1. Compute axon trajectories using __np.gradient()__\n",
    "\n",
    "* Currently using np.gradient() - calculating partial derivatives, uses central differences. \n",
    "* __NB:__ Computes the gradient of the __ENTIRE FIELD__ (your whole 2D array of potential values) __BEFORE__ the neuron starts growing.\n",
    "* np.gradient() gives a smooth, continuous vector pointing in the steepest downhill direction based on real-valued differences.\n",
    "* So np.gradient() lets your axons flow smoothly and follow curving trajectories—not just make stepwise hops.\n",
    "* Good if you __ASSUME__ cues don't change significantly during the short time window of axon outgrowth.\n",
    "\n",
    "\\+ Our model is simplified and doesn't include fluctuations in gradients over time, so this would work and gives smooth paths, potentially faster\n",
    "\n",
    "\\- not suitable for dynamic changes - e.g. if you want to model changes in released molecules over time, e.g. reuptake of some chemical molecules by cells (biologically relevant)\n",
    "\n",
    "##### Potential solutions:\n",
    "* You could re-compute gradients dynamically if cues were changing.\n",
    "* Or compute the gradient on-the-fly using only local sampling at each neuron’s position (e.g., using a 3×3 patch).\n",
    "\n",
    "#### 2. Computing axon trajectories by comparing the 8 neighbours and chosing the lowest value to move toward\n",
    "\n",
    "* That method is discrete and local—it only checks immediate neighbors.\n",
    "* Potentially slower? \n",
    "\n",
    "Why not just “go toward the smaller number”?\n",
    "That’s a simpler rule, and it’s similar to the \"8-neighbor steepest descent\" method we discussed.\n",
    "\n",
    "---\n",
    "### Noise\n",
    "#### Math explainer part if included\n",
    "\n",
    "2. Modeling Biological Noise \n",
    "\n",
    "To simulate the stochastic nature of molecular gradients and sensing noise, we add **Gaussian white noise**:\n",
    "\n",
    "$$\n",
    "F_{\\text{noisy}}(x, y) = F(x, y) + \\eta(x, y), \\quad \\eta(x, y) \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The noise term $\\eta(x, y)$ is sampled independently at each grid point. The amplitude $\\sigma$ controls the noise intensity.\n",
    "\n",
    "---\n",
    "\n",
    "#### Total field noise\n",
    "|Gausian white noise                    |Perlin noise|\n",
    "|---                                    |---            |\n",
    "|Easy to implement                      |X              |\n",
    "|Jerky - steep differences              |Smooth - might more accurately represent tissues|\n",
    "|Random - less danger of bias           |Without biological data to support parameters it <br> might bias the model wrongly|\n",
    "|Mimics highly localized, rapidly changing <br> fluctuations like:<br> * receptor binding noise<br> * protein expression variability <br>     |Tissue-level inhomogeneities| \n",
    "\n",
    "Maybe we can do both?\n",
    "\n",
    "#### Wandering neuron noise?\n",
    "\n",
    "---\n",
    "\n",
    "### Final field - Field superposition\n",
    "\n",
    "Model uses linear superposition - it is a (linear) sum of the 2 repulsive fields and the attractive field\n",
    "\n",
    "\\+ Simple and easy to navigate\n",
    "\n",
    "\\- In biological systems some signals might have limitations in their actions e.g.:\n",
    "* Axons have limited number of receptors that might be saturated and stronger signal wouldn't elicit stronger response.\n",
    "* Some molecules might interact with each other to amplify / diminish response (like cathalizors for chemical reactions)\n",
    "\n",
    "These effects are not accounted for when simply summing the fields.\n",
    "\n",
    "---\n",
    "### Limitations\n",
    "\n",
    "#### Doesn't include axon-axon interaction\n",
    "\n",
    "Axons themselves secrete Netrins which attract axons to each other helping them to form tracts (formations like cables). This could be addressed by introducing a __dynamic attractor field__ that strengthens along paths where multiple axons travel, mimicking this self-reinforcement mechanism.\n",
    "\n",
    "#### Axons move though simulated cell bodies \n",
    "\n",
    "Axons currently pass through simulated cell bodies (lower part of the field). In reality, physical cell exclusion and contact-mediated guidance play important roles. This could be addressed by adding a masking field that represent impenetrable areas, improving spatial realism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b37f11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5d8d8e6",
   "metadata": {},
   "source": [
    "### Mathematical Model of Axon Navigation via Scalar Potential Fields and Gradients\n",
    "\n",
    "Axon guidance during neural development is driven by chemical cues in the extracellular environment. These cues can be modeled mathematically as **scalar potential fields**: regions in space where concentrations of molecular attractants or repellents vary continuously.\n",
    "\n",
    "#### Scalar Field Representation\n",
    "\n",
    "We define a **total potential field** \\( \\phi(x, y) \\in \\mathbb{R} \\) on a 2D spatial domain \\([0,1] \\times [0,1]\\), representing the net effect of attractors (negative potential wells) and repellents (positive barriers). This is computed as a weighted sum of Gaussian sources:\n",
    "\n",
    "\\[\n",
    "\\phi(x, y) = \\sum_{i=1}^{N_\\text{amacrine}} A_i e^{-\\frac{(x - x_i)^2 + (y - y_i)^2}{2\\sigma^2_\\text{amacrine}}}\n",
    "+ \\sum_{j=1}^{N_\\text{muller}} M_j e^{-\\frac{(x - x_j)^2 + (y - y_j)^2}{2\\sigma^2_\\text{muller}}}\n",
    "- \\sum_{k=1}^{N_\\text{attractors}} T_k e^{-\\frac{(x - x_k)^2 + (y - y_k)^2}{2\\sigma^2_\\text{attractor}}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( (x_i, y_i) \\), \\( (x_j, y_j) \\), and \\( (x_k, y_k) \\) are spatial coordinates of **amacrine cells**, **Müller glia**, and **attractors** (e.g. optic disc).\n",
    "- \\( A_i, M_j, T_k \\) are the strength parameters of each cue type.\n",
    "- \\( \\sigma \\) controls the spatial spread (standard deviation) of each field source.\n",
    "\n",
    "This potential field \\( \\phi(x, y) \\) is stored in `total_field` in the simulation code.\n",
    "\n",
    "#### Gradient-Guided Movement\n",
    "\n",
    "Neurons detect spatial differences in cue concentrations and extend their axons in the direction of **steepest descent** of the potential field. Mathematically, this corresponds to following the **negative gradient vector**:\n",
    "\n",
    "\\[\n",
    "\\vec{v}(x, y) = -\\nabla \\phi(x, y) = -\\left( \\frac{\\partial \\phi}{\\partial x}, \\frac{\\partial \\phi}{\\partial y} \\right)\n",
    "\\]\n",
    "\n",
    "The gradient field \\( \\nabla \\phi \\) is computed using `np.gradient` in the simulation code. Each axon's position \\( \\vec{r}(t) = (x(t), y(t)) \\) is updated iteratively using:\n",
    "\n",
    "\\[\n",
    "\\vec{r}(t+1) = \\vec{r}(t) + \\Delta t \\cdot \\vec{v}(x(t), y(t))\n",
    "\\]\n",
    "\n",
    "This corresponds to a first-order **Euler integration** of the continuous gradient descent dynamics.\n",
    "\n",
    "#### Discretization and Implementation\n",
    "\n",
    "In code:\n",
    "- The scalar field \\( \\phi(x, y) \\) is discretized on a 2D grid (`grid_width` × `grid_height`).\n",
    "- Gradients are approximated using finite differences via `np.gradient`.\n",
    "- The neuron’s path is computed step-by-step by sampling the gradient at the nearest grid point and moving along the normalized negative gradient vector.\n",
    "- If the gradient is zero (i.e., flat region), a random direction is chosen to mimic stochastic cell behavior.\n",
    "\n",
    "#### Sources of Error and Biological Simplifications\n",
    "\n",
    "1. **Discrete approximation**:  \n",
    "   - The gradient is calculated on a grid; thus, resolution affects accuracy.\n",
    "   - Euler integration can introduce errors, especially near steep gradients.\n",
    "\n",
    "2. **Biological realism**:  \n",
    "   - Real axons respond to multiple interacting guidance cues over time, not instantaneously.\n",
    "   - Temporal changes and cell signaling cascades are not modeled here.\n",
    "\n",
    "3. **Noise/randomness**:  \n",
    "   - To simulate biological variability, randomness is introduced in starting positions and in movement when gradients are zero.\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "- The **scalar potential** is visualized using a **heat map** (`imshow`) to represent concentration intensity.\n",
    "- The **vector field** \\( \\nabla \\phi \\) can optionally be visualized using arrows (`quiver`) to illustrate direction and magnitude of axonal force.\n",
    "- The **trajectories** of axons are shown as lines starting from initial positions and tracing their paths under gradient influence.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This model abstracts axon guidance as **gradient descent in a scalar field**, where chemical cues act as potential wells or barriers. While simplified, it captures key mathematical principles of navigation, optimization, and vector calculus, and provides a basis for simulating biologically inspired agent-based systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
